After hours of research and testing, I assumed that SVC could be my potential choice. I choose the SVC since we have
a lot of important features that need to be used in the predictiion. Working with the provided code and the RFC model,
model itself gave me a lot of questions. When talking about the models and measures, I have to admit that the provided model
was one of the best models out of the list, however, the set up was very simple. I know that this task checks the
understanding of the code and models, however, in order to perform the best tests possible, we should dive in deeper
into the settings and make sure that the base for those models is tuned up.
After exploring the data and brief analysis we can state that there are missing values in the test data.
Age and embarked columns were the most significant ones.
(As it was in the initianl files, the rows were ignored in the porcess.
I would somehow agree. We could potentialy fill those gaps with median for age and just picked one emabrked value
for embarked column, however, from lager perspective that step would affect the final result in some sort of negative way.
the age could be replaced with median
data['Age'].fillna(data['Age'].median(), inplace=True) => that gives 125 more rows that could go into the training
Howerever, I belive that Age is very important feature and if you really wanna see if you would surive or not, age should
be require in the prediction.
)
Just by looking at the documentation for each option, I can state that there is a lot of settings that can be done for
each model. I know that we have to search based on our provided data, however, there is so many options that people
tried to perform and the accuracy in those tests were different by vary small one digit number.
So, going back to my chosen model, I just went ahead and changed the build feature to make sure that we have our data
represented in binary structure which would fit both models, RFC and SVC.
(I am always trying to put all data in binary for machine learning)
After setting up both training and predict function, I got quite satisfying results.
(NA or NULL entries were ignored)

RFC:

Train accuracy for the model is 0.896124031007752
Classification report for the model:
               precision    recall  f1-score   support

           0       0.89      0.95      0.92       385
           1       0.91      0.82      0.86       260

    accuracy                           0.90       645
   macro avg       0.90      0.88      0.89       645
weighted avg       0.90      0.90      0.90       645

Prediction:
Accuracy is 0.7611940298507462

SVC:

Train accuracy for the model is 0.8294573643410853
Classification report for the model:
               precision    recall  f1-score   support

           0       0.83      0.90      0.86       385
           1       0.83      0.73      0.78       260

    accuracy                           0.83       645
   macro avg       0.83      0.81      0.82       645
weighted avg       0.83      0.83      0.83       645

Prediction:
Accuracy is 0.8208955223880597


Going back to the full set up.
Every single function is in the seprate file. Main.py is the main functions that does all the work (api).
Kept the files (preprocess and build features) separated to make sure that we can reuse them or modify specific part only.
(Those two could be potentially combined)
With that being said we can simply attach that model to the main.py and have them both runnig at the same time.
There are many ways that can be applied to that application. This model also can be incorporated in the train file
in the src directory and both results can be returned in the predict function. It is just the matter of the set up.



